{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c35593e-5cba-441b-9f21-b585951ff2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([6, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# 单通道卷积做辅助\n",
    "def Conv2d_single(X,K):\n",
    "    h,w =K.shape #卷积核的高宽\n",
    "    Y =torch.zeros(X.shape[0] - h + 1, X.shape[1] - w +1)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            region = X[i:i + h, j:j + w]\n",
    "            Y[i,j] = (region * K).sum()\n",
    "    return Y\n",
    "#多通道卷积    \n",
    "class Conv2d:\n",
    "    def __init__(self, in_c, out_c, k_size=3):\n",
    "        #权重是每个输出通道有in_c个kernel，而每个kernel有3*3个参数\n",
    "        self.weight = torch.randn(out_c, in_c, k_size, k_size)*0.1  # 权重怎么初始化？\n",
    "        #bias每个输出通道一个即可\n",
    "        self.bias = torch.randn(out_c)*0.1    # 偏置也很重要哦！\n",
    "        \n",
    "    def forward(self, X):\n",
    "        in_c,H,W =X.shape\n",
    "        out_c,_,k_h, k_w =self.weight.shape #一一对应\n",
    "        H_out, W_out = H - k_h +1, W - k_w +1 #计算输出的高宽\n",
    "        Y = torch.zeros((out_c,H_out,W_out)) #初始化\n",
    "\n",
    "        for oc in range (out_c):# 遍历输出通道\n",
    "            y= torch.zeros((H_out,W_out))\n",
    "            for ic in range(in_c):# 遍历输入通道\n",
    "                y+= Conv2d_single(X[ic],self.weight[oc,ic])\n",
    "            Y[oc]= y+self.bias[oc]\n",
    "        return Y\n",
    "\n",
    "\n",
    "#测试一下\n",
    "if __name__ == \"__main__\":\n",
    "    X = torch.randn(3, 4, 5)   \n",
    "    conv = Conv2d(in_c=3, out_c=6, k_size=3)\n",
    "    Y = conv.forward(X)\n",
    "    print(X.shape)\n",
    "    print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ca2bb6-0a68-445b-86fb-493f239b210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Maxpool2d (X,pool_size= 2,stride =2):\n",
    "    h,w = pool_size, pool_size\n",
    "    #计算输出高宽\n",
    "    out_h = (X.shape[0]-h+stride)/stride\n",
    "    out_w = (X.shape[0]-w+stride)/stride\n",
    "    Y = torch.zeros ((out_h, out_w))\n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            #卷积核扫描过的区域的高宽\n",
    "            region = X[i*striede : i*stride + h, j*stride: j*stride +h ]\n",
    "            Y[i,j] =region.max()#对此区域取最大值\n",
    "    return Y \n",
    "\n",
    "def Avgpool2d (X,pool_size=2,stride=2):#同理\n",
    "    h,w = pool_size,pool_size\n",
    "    out_h = (X.shape[0]-h+ stride)/stride\n",
    "    out_w = (X.shape[1]- h +stride)/stide\n",
    "    Y = torch.zeros ((out_h, out_w))\n",
    "    for i in range(out_h):\n",
    "        for j in range (out_w):\n",
    "            region = X[i*stride:i*stride+h, j*stride:j*stride +w]\n",
    "            Y[i,j] = region.mean ()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e27a8e8a-d4b3-410a-bbeb-b8499c8808de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#动手实现batchnorm\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self,num_features,num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:#维度基本就是在2（全连接层）和4（卷积层）之间\n",
    "            shape = (1,num_features)\n",
    "        else:\n",
    "            shape =(1,num_features,1,1)#BN对卷积层把通道当作特征学习参数\n",
    "        #可学习的参数，对应的是新的均值和方差\n",
    "        self.gamma = nn.Parameter(torch.ones(shpae))#新方差，所以不为0\n",
    "        self.beta = nn.Paraeter(torch.zeros(shape))\n",
    "        #定义可改变的均值和方差\n",
    "        self.moving_mean= torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "\n",
    "    def forward(self,X) :\n",
    "        if self.training:\n",
    "            #定义全局的均值和方差\n",
    "            mean = X.mean(dim=0, keepdim=Ture) #对当前小批量样本数据算\n",
    "            var = ((X -mean)**2).mean(dim=0,keepdim=True)\n",
    "            self.moving_mean = 0.9 * self.moving_mean + 0.1 * mean\n",
    "            self.moving_var = 0.9 * self.moving_var + 0.1*var\n",
    "        else:\n",
    "            mean,var = self.moving_mean, self.moving_var\n",
    "\n",
    "        X_hat = (X-mean)/ torch.sqrt(var + 1e-5) #加一个eps噪音\n",
    "        return self.gamma * X_hat + self.beta\n",
    "        #通过随机的缩放和偏差来控制模型复杂度\n",
    "        #保证了既有随机性又不会变太剧烈（0.9的lr使self.moving变得比较慢）\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41d3ae28-1512-4bf5-b106-2e570e37d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "#首先构建一个vgg block\n",
    "class vgg_block(nn.Module):\n",
    "    #设定的参数包括输入输出通道和卷积层个数\n",
    "    def __init__ (self,in_channels, out_channels,num_convs):\n",
    "        super().__init__()\n",
    "        layers =[]\n",
    "        for i in range (num_convs):\n",
    "            layers.append(nn.Conv2d(in_channels if i == 0 else out_channels,out_channels,kernel_size =3,padding = 1 ))\n",
    "            layers.append(nn.ReLU(inplace=True)) #加一个非线性激活函数\n",
    "        layers.append(nn.MaxPool2d(kernel_size=2,stride=2))#最大池化层\n",
    "        self.block=nn.Sequential(*layers)\n",
    "\n",
    "def forward(self,x):\n",
    "    return self.block(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c9c1f8e-56f2-437e-8a04-524cec7c1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg16(nn.Module):\n",
    "    def __init__(self,num_classes=1000):#以imagenet为例，千分类\n",
    "        super().__init__()\n",
    "        #16=2+2+3+3+3(卷积层)+3(全连接)#设置每一个block的通道数和cv层个数\n",
    "        self.features=nn.Sequential(vggblock(3,64,2),vggblock(64,128,2),vggblock(128,256,3),vggblock(256,512,3),vggblock(512,512,3))\n",
    "        self.classifier = nn.Sequential(nn.Flatten(),nn.linear(512*7*7,4096),nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(4096,4096),nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(4096,num_classes)\n",
    "                                       )#最后加3个全连接层\n",
    "        def forward(self,x):\n",
    "            x=self.features(x)\n",
    "            x= self.classifier(x)\n",
    "            return x\n",
    "        #Pytorch会自动构建计算图，完成反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a26f3-482e-4964-b5d4-489c9cf79ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a72ca0-6a01-4590-b23b-781ce69c49d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df2d147-e6c1-46c0-93ea-fee5965b6a00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
