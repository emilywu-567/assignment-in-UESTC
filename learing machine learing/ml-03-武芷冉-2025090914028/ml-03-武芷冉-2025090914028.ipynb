{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a551ddb2-e1d5-4f40-8361-6438ee76a697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从file://national-survey-on-school-dropout.zip下载data\\national-survey-on-school-dropout.zip...\n",
      "正在从本地复制 national-survey-on-school-dropout.zip 到 data\\national-survey-on-school-dropout.zip...\n",
      "   FOLIO  q_personas  q_hombres  q_mujeres  x<30   pc  laptop  tv_plana  \\\n",
      "0      1           1          1          0     2  0.0     0.0       0.0   \n",
      "1      2           4          2          2     1  1.0     1.0       1.0   \n",
      "2      3           2          0          2     2  0.0     0.0       0.0   \n",
      "3      4           1          0          1     2  0.0     0.0       0.0   \n",
      "4      5           2          1          1     2  0.0     0.0       0.0   \n",
      "\n",
      "   tablet  smartphone  InternetF  sIF_causa  mejora_vida  mejora_trabajo  \\\n",
      "0     0.0         0.0        0.0        0.0          0.0             0.0   \n",
      "1     1.0         1.0        1.0        0.0          1.0             1.0   \n",
      "2     0.0         0.0        0.0        0.0          0.0             0.0   \n",
      "3     0.0         0.0        0.0        0.0          0.0             0.0   \n",
      "4     0.0         0.0        0.0        0.0          0.0             0.0   \n",
      "\n",
      "   mejor_decisor  ENT  FACTOR  \n",
      "0            0.0    1     925  \n",
      "1            1.0   24     809  \n",
      "2            0.0    2    1248  \n",
      "3            0.0    9    8713  \n",
      "4            0.0    5    1292  \n",
      "   FOLIO  SEXO  EDAD  edu_inicial  inscrito  nivel_edu  terminado  nt_causa  \\\n",
      "0      2     1    13          0.0       1.0        3.0        1.0       0.0   \n",
      "1      2     2    19          0.0       1.0        9.0        1.0       0.0   \n",
      "2      7     2     8          0.0       1.0        3.0        1.0       0.0   \n",
      "3      7     1    26          0.0       2.0        0.0        0.0       0.0   \n",
      "4      9     1    27          0.0       1.0        9.0        1.0       0.0   \n",
      "\n",
      "   asesorias  extraord  ...  tv_plana  tablet  smartphone  InternetF  \\\n",
      "0        2.0       0.0  ...       1.0     1.0         1.0        1.0   \n",
      "1        1.0       2.0  ...       1.0     1.0         1.0        1.0   \n",
      "2        2.0       0.0  ...       1.0     2.0         1.0        1.0   \n",
      "3        0.0       0.0  ...       1.0     2.0         1.0        1.0   \n",
      "4        2.0       2.0  ...       1.0     1.0         1.0        1.0   \n",
      "\n",
      "   sIF_causa  mejora_vida  mejora_trabajo  mejor_decisor  ENT  FACTOR_y  \n",
      "0        0.0          1.0             1.0            1.0   24       809  \n",
      "1        0.0          1.0             1.0            1.0   24       809  \n",
      "2        0.0          2.0             2.0            3.0   22       722  \n",
      "3        0.0          2.0             2.0            3.0   22       722  \n",
      "4        0.0          1.0             1.0            1.0   26       425  \n",
      "\n",
      "[5 rows x 67 columns]\n",
      "Epoch [1,Train Loss: 0.1440,Train Accuracy: 97.16%, Test Accuracy: 99.91%\n",
      "Epoch [11,Train Loss: 0.0549,Train Accuracy: 98.93%, Test Accuracy: 100.00%\n",
      "Epoch [20,Train Loss: 0.1347,Train Accuracy: 96.21%, Test Accuracy: 99.99%\n"
     ]
    }
   ],
   "source": [
    "import hashlib #通过计算文件SHA1，检验完整性\n",
    "import os\n",
    "import tarfile\n",
    "import zipfile #解压缩\n",
    "import requests#用于下载文件\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn #神经网络模块\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split #用于训练集数据集划分\n",
    "import torch.optim as optim #优化模块\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#以下哈希验证是从李沐的书里学习的，重叠度比较高哈（）\n",
    "DATA_HUB = dict() #创建字典\n",
    "DATA_HUB = { #定义URL\n",
    "    'national-survey-on-school-dropout': (\n",
    "        'file://national-survey-on-school-dropout.zip',  # 使用file://协议指向本地文件None  # 没有SHA1哈希值设为None\n",
    "        None\n",
    "    )\n",
    "}\n",
    "\n",
    "def download(name, cache_dir=os.path.join('data')):\n",
    "    #下载一个DATA_HUB中的文件，返回本地文件名\n",
    "    assert name in DATA_HUB, f\"{name} 不存在于 {DATA_HUB}\"\n",
    "    url, sha1_hash = DATA_HUB[name]\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    \n",
    "    # 检查文件是否已存在且哈希匹配\n",
    "    if os.path.exists(fname) and sha1_hash:\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, 'rb') as f: #以二进制读取\n",
    "            while True:\n",
    "                data = f.read(1048576) #每次读1MB\n",
    "                if not data: \n",
    "                    break #直至读完\n",
    "                sha1.update(data) #更新哈希值\n",
    "        if sha1.hexdigest() == sha1_hash:\n",
    "            return fname  #文件完整，返回 \n",
    "    \n",
    "    print(f'正在从{url}下载{fname}...')\n",
    "    \n",
    "    # 处理本地文件\n",
    "    if url.startswith('file://'):\n",
    "        local_path = url[7:]  # 移除 file://得到实际路径\n",
    "        print(f'正在从本地复制 {local_path} 到 {fname}...')\n",
    "        \n",
    "        # 确保源文件存在\n",
    "        if not os.path.exists(local_path):\n",
    "            raise FileNotFoundError(f\"本地文件不存在: {local_path}\")\n",
    "            \n",
    "        # 复制文件\n",
    "        import shutil\n",
    "        shutil.copy2(local_path, fname)\n",
    "    else:\n",
    "        # 处理网络URL，下载\n",
    "        r = requests.get(url, stream=True, verify=True)\n",
    "        with open(fname, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    \n",
    "    return fname\n",
    "\n",
    "#@save\n",
    "def download_extract(name, folder=None):\n",
    "    #下载并解压zip/tar文件\n",
    "    fname = download(name)\n",
    "    base_dir = os.path.dirname(fname) #创建一个base目录\n",
    "    data_dir, ext = os.path.splitext(fname)\n",
    "    \n",
    "    if ext == '.zip':\n",
    "        fp = zipfile.ZipFile(fname, 'r')\n",
    "    elif ext in ('.tar', '.gz'):\n",
    "        fp = tarfile.open(fname, 'r')\n",
    "    else:\n",
    "        assert False\n",
    "    \n",
    "    fp.extractall(base_dir) #解压到base目录\n",
    "    return os.path.join(base_dir, folder) if folder else data_dir\n",
    "\n",
    "def download_all():\n",
    "    #下载DATA_HUB中的所有文件\n",
    "    for name in DATA_HUB:\n",
    "        download(name)\n",
    "        \n",
    "#接受内函数返回值，定义一个全局变量        \n",
    "data_dir = download_extract('national-survey-on-school-dropout')\n",
    "\n",
    "# 加载两个CSV文件\n",
    "df_vivienda = pd.read_csv(os.path.join( data_dir,'df01_TipoVivienda.csv'))\n",
    "df_enape = pd.read_csv(os.path.join(data_dir, 'df03_Dataset_ENAPE.csv'))\n",
    "\n",
    "# 查看数据的前几行\n",
    "print(df_vivienda.head())\n",
    "print(df_enape.head())\n",
    "\n",
    "#依据数据集中的 'folio'特征，对df_01 和df_03 进行融合\n",
    "merged_df = pd.merge(df_vivienda, df_enape, on='FOLIO', how='inner')\n",
    "\n",
    "#选择数据集中相关的特征用于训练\n",
    "features = [\n",
    "    'EDAD', 'nivel_edu', 'estres', 'depresion', 'desespero', \n",
    "    't_smartphone', 't_laptop', 't_PC', 't_tablet', 't_TV',\n",
    "    'q_personas_x', 'q_hombres_x', 'q_mujeres_x', 'x<30_x', 'pc_x', \n",
    "    'laptop_x', 'tv_plana_x', 'tablet_x', 'smartphone_x','inscrito'\n",
    "]\n",
    "#定义terminado 为决定学生是否退学的目标特征\n",
    "target = 'terminado'\n",
    "\n",
    "#用所选特征创建最终的数据集\n",
    "final_df = merged_df[features + [target]]\n",
    "#删除数据集中所有target=2的行（我观察到数据有上万个，因此删除）\n",
    "final_df = final_df[final_df[target] != 2]\n",
    "\n",
    "\n",
    "#用同一列的均值替换数据中的\"9\"项\n",
    "for column in final_df.columns:\n",
    "    if final_df[column].dtype in [np.int64, np.float64] and column != target:\n",
    "        #计算均值\n",
    "        col_mean = final_df[final_df[column] != 9][column].mean()\n",
    "        #将9替换为均值\n",
    "        final_df[column] = final_df[column].replace(9, col_mean)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "#分离目标和特征\n",
    "X = final_df.drop(columns=[target]) #从final_df 只删除目标列\n",
    "y = final_df[target] #只保留目标列\n",
    "\n",
    "#使所有特征具有相同的尺度，使不同特征中0-9绝对数值的大小不会影响模型的拟合\n",
    "#转化成正态分布\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#将数据集按 8：2比例分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,random_state=42)\n",
    "\n",
    "# 将DataFrame转换为NumPy数组，再转成PyTorch张量\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "#创建DataLoader用于按批次加载数据，进行小批量训练\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "#设置MLP分类模型\n",
    "class DropoutClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, output_size=2):\n",
    "        super(DropoutClassifier, self).__init__()\n",
    "        #设置了2个隐藏层，设置了它们的大小\n",
    "        self.fc1= nn.Linear(input_size, hidden_size)\n",
    "        #self.fc2= nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2= nn.Linear(hidden_size, output_size)\n",
    "        self.dropout= nn.Dropout(0.5)  # 暂退法防止过拟合\n",
    "    #前向传播\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # 应用 dropout \n",
    "        #x = torch.relu(self.fc2(x)) #用了两个隐藏层 accuracy直接100%了，于是删了一个\n",
    "        #x = self.dropout(x)  # 再用 dropout \n",
    "        x = self.fc2(x) \n",
    "        return x\n",
    "        \n",
    "#初始化模型\n",
    "model = DropoutClassifier(input_size=X_train.shape[1])\n",
    "\n",
    "#设置loss函数,优化（用adam自适应），学习率\n",
    "criterion = nn.CrossEntropyLoss()  # 针对二分类\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "#训练模块 （大框架不变）\n",
    "def train(model, train_loader, test_loader, criterion, optimizer, epochs=50,lr=0.1):\n",
    "    for epoch in range(epochs):\n",
    "        model.train() #训练模式启动 \n",
    "        running_loss = 0.0 #累计损失\n",
    "        correct_preds = 0 #正确预测的个数\n",
    "        total_preds = 0 #总预测的个数\n",
    "        \n",
    "        #用训练集评价模型\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()  # 清空梯度\n",
    "            outputs = model(inputs) #前向传播路径\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()  # 反向传播更新梯度\n",
    "            optimizer.step()  \n",
    "            \n",
    "            # 计算准确率\n",
    "            _, predicted = torch.max(outputs, 1)#筛选“概率”最大值来预测\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            #正确预测个数（True为1，False为0）累加即可求得\n",
    "            total_preds += labels.size(0)#预测总数\n",
    "            running_loss += loss.item() \n",
    "        #计算 \n",
    "        train_accuracy = 100 * correct_preds / total_preds\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        #用测试集评价模型（和上一个逻辑相同）\n",
    "        model.eval()  #启动锐评模式\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "        with torch.no_grad(): #不要再更新梯度了\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_preds += (predicted == labels).sum().item()\n",
    "                total_preds += labels.size(0)\n",
    "\n",
    "        test_accuracy = 100 * correct_preds / total_preds\n",
    "\n",
    "        #每10个循环一打印\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f'Epoch [{epoch+1},Train Loss: {avg_train_loss:.4f},Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "train(model, train_loader, test_loader, criterion, optimizer, epochs=20)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac1e303-a78f-4180-aa4c-e4b383f509fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
